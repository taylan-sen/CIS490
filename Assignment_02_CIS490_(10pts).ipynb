{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 02 - CIS490 (10pts)",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOzIOE2xk20u1EiAXBgaqRQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taylan-sen/CIS490/blob/main/Assignment_02_CIS490_(10pts).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##(3 pts) Short questions\n",
        "\n",
        "Fill in short definitions/answers for each of the following:\n",
        "\n",
        "**observation space** -  \n",
        "**action space** -  \n",
        "**agent** -  \n",
        "\n",
        "For MountainCar,  \n",
        "* Is the observation space continuous or discrete?  \n",
        "* How many dimensions is it? \n",
        "\n",
        "* Is the action space continuous or discrete?"
      ],
      "metadata": {
        "id": "ncGUy0G9lrPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (3pts) Open loop agent. \n",
        "In the following cell, implement your open loop agent. You must not use the observation return variable. I will run it ten times to determine the average number of iterations to reach the top. (The lowest average in the class gets a bonus point.)"
      ],
      "metadata": {
        "id": "1AL91QWXdHSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell once to allow playable videos to be created\n",
        "# Make sure it says SUCCESS after running, or rerun.\n",
        "\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "# install some helper functions to allow playing render videos in colab\n",
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment \n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env\n",
        "\n",
        "print('SUCCESS!')"
      ],
      "metadata": {
        "id": "CuHe7CrSo5Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR OPEN-LOOP AGENT GOES HERE (YOU SHOULD NOT USE observation FOR THE\n",
        "# OPEN LOOP AGENT)\n",
        "env = wrap_env(gym.make(\"MountainCar-v0\"))\n",
        "observation = env.reset()\n",
        "# observation is a two dimensional tuple that contains the position and \n",
        "# speed. The initial position and speed is not (0,0) but rather there is\n",
        "# a small random starting position and speed.\n",
        "\n",
        "step_i = 0  # use this variable as a counter for each step taken\n",
        "\n",
        "while True:    \n",
        "    env.render()\n",
        "    \n",
        "    #--------------------------------------------\n",
        "    # your agent goes here (change the code here)\n",
        "    # currently the action is randomly sampled from the action space\n",
        "    action = env.action_space.sample()\n",
        "    #-------------------------------------------\n",
        "\n",
        "    # The following line calls the environment simulator with the\n",
        "    # specified action. It runs the physics simulator for one time\n",
        "    # step, and returns the observation, reward, done, and info. \n",
        "    # For our purposes, we are only interested in observation and done.\n",
        "    # done is a bool, that is true if the car reaches the flag and is\n",
        "    # false otherwise. \n",
        "    observation, reward, done, info = env.step(action) \n",
        "    step_i = step_i + 1\n",
        " \n",
        "    # if car reaches the flag, exit the loop\n",
        "    if done: \n",
        "      break;\n",
        "\n",
        "print('------------------------\\nDONE')\n",
        "print('# steps taken: ', step_i)\n",
        "env.close()\n",
        "show_video()\n"
      ],
      "metadata": {
        "id": "wqBIzit6r08l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (4pts) Closed-loop agent. \n",
        "In the following cell, implement your closed-loop agent. You must make use of the observation variable in a meaningful way. I will run it ten times to determine the average number of iterations to reach the top. (The lowest average in the class gets a bonus point.)"
      ],
      "metadata": {
        "id": "NkVxRQTLd0tR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CLOSED LOOP AGENT (use observation)\n",
        "# Insert code for your closed loop agent here (i.e. you should use \n",
        "# observation for deciding your next action).\n",
        "# To start out, copy and paste the entire code contents of the above cell here."
      ],
      "metadata": {
        "id": "Hy6xe6Yhudr2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}